# Image Captioning Project

An end-to-end deep learning system for automatic image captioning using an encoder-decoder architecture with attention mechanism. This project uses a CNN (ResNet) encoder and an LSTM decoder with attention to generate natural language descriptions for images.

## Project Structure

- `models.py`: Contains neural network model definitions
  - `Encoder`: CNN-based encoder using ResNet backbone
  - `DecoderWithAttention`: LSTM-based decoder with attention mechanism

- `dataset.py`: Dataset handling and preprocessing
  - `FlickrDataset`: Dataset class for the Flickr30k dataset
  - Vocabulary generation and text preprocessing

- `utils.py`: Utility functions
  - `CapCollat`: Custom collation function for batching
  - Support functions for checkpoint handling and evaluation metrics

- `train.py`: Training script with validation functionality
  - Training loop with gradient clipping
  - Periodic validation with BLEU score computation

- `test.py`: Evaluation script for model performance
  - Calculation of BLEU metrics on test set
  - Sample caption generation and visualization

- `inference.py`: Inference script for new images
  - Generate captions for new images
  - Visualize attention weights (optional)

- `proeblematic_captions.py`: Script to identify problematic captions in the dataset

## Requirements

- Python 3.6+
- PyTorch 1.7+
- torchvision
- Pillow
- matplotlib
- numpy
- tqdm
- nltk (for BLEU score calculation)

## Setup

1. Install the required packages:
```bash
pip install torch torchvision Pillow matplotlib numpy tqdm nltk
```

2. Download the Flickr30k dataset and place images in a directory

3. Ensure caption CSV file is available at the specified location

## Training

To train the model from scratch:

```bash
python train.py --image_dir /path/to/flickr30k_images/ --captions_file /path/to/results.csv --batch_size 32 --epochs 10
```

Key parameters:
- `--image_dir`: Directory containing the images
- `--captions_file`: Path to the CSV file with captions
- `--lr`: Learning rate (default: 4e-4)
- `--batch_size`: Batch size (default: 32)
- `--epochs`: Number of training epochs (default: 10)
- `--checkpoint_dir`: Directory to save model checkpoints (default: 'checkpoints')
- `--grad_clip`: Gradient clipping value (default: 5.0)

## Evaluation

To evaluate a trained model on the test set:

```bash
python test.py --image_dir /path/to/flickr30k_images/ --captions_file /path/to/results.csv --checkpoint /path/to/checkpoint.pth --full_test
```

Key parameters:
- `--checkpoint`: Path to a specific model checkpoint
- `--full_test`: Flag to run evaluation on the entire test set
- `--output_dir`: Directory to save output visualizations (default: 'output')
- `--beam_size`: Beam size for caption generation (default: 3)

## Inference

To generate captions for a single image:

```bash
python inference.py --image_path /path/to/image.jpg --checkpoint /path/to/checkpoint.pth
```

Key parameters:
- `--image_path`: Path to the input image
- `--visualize_attention`: Flag to visualize attention weights
- `--beam_size`: Beam size for caption generation (default: 3)
- `--output_dir`: Directory to save visualizations (default: 'output')

## Examples

### Sample Generated Captions

Here are some examples of captions generated by the model:

- "a man in a blue shirt is playing guitar on a stage"
- "two children are playing in the snow with their dog"
- "a woman in a red dress is walking down the street"

## Model Architecture

### Encoder
- ResNet-based CNN encoder (default: ResNet-101)
- Output feature map dimensions: 14x14x2048
- Fine-tuning of later layers for adaptation to the captioning task

### Decoder
- LSTM-based decoder with attention mechanism
- Additive attention over image features
- Beam search for caption generation
- Word embeddings learned from scratch

## Performance

The model achieves the following BLEU scores on the Flickr30k test set:

- BLEU-1: ~0.65
- BLEU-2: ~0.46
- BLEU-3: ~0.31
- BLEU-4: ~0.21

## Acknowledgments

This implementation draws inspiration from:
- "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention" by Xu et al.
- The Flickr30k dataset